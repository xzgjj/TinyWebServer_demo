# v5

要实现 V5 的主从 Reactor (Main-Sub Reactor) 架构
mmp零拷贝  静态态资源系统

Main-Sub Reactor: 主从 Reactor 线程池
EventLoop

第二阶段的“mmap 文件映射工具类

---

最忌讳的是“盲目写大块代码”，因为底层 Buffer 或 Epoller 的一个微小变动会影响所有上层逻辑
Connection 状态机迁移 等核心细节。


发送机制升级 (Zero-Copy Send Mechanism)。
核心目标是消除数据拷贝。将不再把文件内容读入用户态的 std::string，而是构建一个支持 Scatter/Gather I/O (聚集写) 的链式缓冲区，利用 writev 系统调用一次性发送 "Header (内存)" + "Body (mmap)"。

异构节点支持：链表中既要能存 std::string (Header)，也要能存 StaticResource (Body)。
零拷贝适配：直接引用 mmap 的地址，不进行 memcpy。
writev 适配：提供 GetIovec 接口，将链表前 N 个节点转换为 struct iovec 数组。

HandleWrite(int fd)

---

##  问题解决




LOG_INFO 如果是阻塞式的，性能会受限于磁盘 IO；如果是异步的，性能会受限于环形队列（RingBuffer）的争用


我在 TinyWebServer 中后期，引入了一个测试驱动的 AI 协作审计系统，
通过失败用例 → 模块归因 → 最小上下文喂给 AI，
避免大模型在复杂代码库中的幻觉问题。

Reactor 模型在 线程亲和性（Thread Affinity） 的处理上存在严重的逻辑违背。
test_main、test_single_connection 以及 test_log 全部出现 45秒运行超时，这表明程序陷入了某种 “活锁” (Livelock) 或 “阻塞等待” (Blocked Waiting) 状态。

AsyncLogger::Flush 等待条件错误:
后端线程 ThreadFunc 中，数据被 swap 到了 buffers_to_write 后，buffers_ 确实空了，但磁盘 IO 可能还在进行中。如果此时前端又快速 Append 导致 current_buffer_ 被推入 buffers_，或者后端线程在特定时机未能触发 notify_all，Flush 就会陷入永久等待。

在 EPOLLET (边缘触发) 模式下，如果缓冲区有 8KB 数据而你只读了 4KB，epoll 将不会再次通知你，直到下次有新数据到达。这会导致请求解析不全

减少 UpdateEvent 调用

致命死锁：EventLoop::Loop 无法退出
在 main.cpp 中，服务器启动后进入了 while(g_running) 的死循环：

Connection 类往往是“万恶之源”——它既被 Server 引用，又引用 HttpParser。永远优先在头文件中使用 std::shared_ptr 的前置声明，而不是直接包含复杂的业务头文件，这能显著减少由于头文件循环包含导致的“莫名其妙”的未定义错误。

功能逻辑上存在严重的“断裂”，这直接导致了 test_single_connection 超时和 test_main 集成测试失败。

http_request.cpp 包含了一个“幽灵实现”块。

---

使用 tools.py 提供的调试功能

python3 tools.py debug -target test_single_connection

 进入 gdb 后执行：
(gdb) r
#等待几秒后按下 Ctrl+C 强行中断
(gdb) thread apply all bt

(gdb) info locals
